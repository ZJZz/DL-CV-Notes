# Tricks

[Ten Techniques Learned From fast.ai](https://blog.floydhub.com/ten-techniques-from-fast-ai/)

[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/pdf/1812.01187.pdf) - [reddit](https://www.reddit.com/r/MachineLearning/comments/a4dxna/r_bag_of_tricks_for_image_classification_with/)

[Improving deep learning models with bag of tricks](https://github.com/kmkolasinski/deep-learning-notes/tree/master/seminars/2018-12-Improving-DL-with-tricks) - [reddit](https://www.reddit.com/r/MachineLearning/comments/a5s8pv/r_a_bags_of_tricks_which_may_improve_deep/)


## Differential Learning rates

## cyclical learning rates

## Cosine annealing

## Stochastic Gradient Descent with restarts

## Anthropomorphise your activation functions

## A game-winning bundle: building up sizes, dropout and TTA

Dropout

Another incredibly simple and effective method they used for tackling overfitting and improving accuracy is training on smaller image sizes, then increasing the size and training the same model on them again.

test time augmentation(TTA)
